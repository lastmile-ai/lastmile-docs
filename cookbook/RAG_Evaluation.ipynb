{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a Retrieval-Augmented Generation (RAG) System\n",
    "\n",
    "This notebook provides a quick start guide on evaluating RAG systems using the Lastmile AutoEval SDK. We'll cover how to:\n",
    "\n",
    "1. Measure the faithfulness of a generated output against a ground truth and input\n",
    "2. Check the toxicity of an input and deny requests that are too toxic \n",
    "3. Detect hallucinations by comparing generated output to retrieved context\n",
    "\n",
    "Note: you will need an OpenAI API key set in your environment variables to run this notebook.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "%pip install \"llama-index>=0.11.0\"\n",
    "%pip install lastmile\n",
    "%pip install pandas\n",
    "\n",
    "# Setup Pandas to display without truncation (for display purposes)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LlamaIndex RAG System\n",
    "\n",
    "First, let's setup a simple RAG system using LlamaIndex to generate responses. \n",
    "Note: llama index defaults to using OpenAI as the LLM, But feel free to swap it out with your preferred LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data/PaulGrahamEssay\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating With Base Metric Faithfulness \n",
    "\n",
    "Now let's evaluate how faithful the generated response is to the ground truth, given an input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evlauation results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>Faithfulness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where did the author grow up?</td>\n",
       "      <td>The author grew up in Yorkville, a neighborhoo...</td>\n",
       "      <td>England</td>\n",
       "      <td>0.058303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           input  \\\n",
       "0  Where did the author grow up?   \n",
       "\n",
       "                                              output ground_truth  \\\n",
       "0  The author grew up in Yorkville, a neighborhoo...      England   \n",
       "\n",
       "   Faithfulness_score  \n",
       "0            0.058303  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lastmile.lib.auto_eval import AutoEval, Metric\n",
    "import pandas as pd\n",
    "\n",
    "eval = AutoEval()\n",
    "\n",
    "query = \"Where did the author grow up?\"\n",
    "expected_response = \"England\"\n",
    "llm_response = query_engine.query(query)\n",
    "\n",
    "eval_result = eval.evaluate_data(\n",
    "    data=pd.DataFrame({\n",
    "        \"input\": [query],\n",
    "        \"output\": [llm_response.response],\n",
    "        \"ground_truth\": [expected_response]\n",
    "    }),\n",
    "    metrics=[Metric(name=\"Faithfulness\")]\n",
    ")\n",
    "\n",
    "print(f'Evaluation results:')\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Output Toxicity\n",
    "\n",
    "We can also check the toxicity of the output and deny responses that are too toxic. Designed to detect and flag low-quality or potentially harmful AI-generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity: 0.7909269332885742\n"
     ]
    }
   ],
   "source": [
    "toxicity_result = eval.evaluate_data(\n",
    "    data=pd.DataFrame({\"output\": [llm_response.response]}),\n",
    "    metrics=[Metric(name=\"Toxicity\")]\n",
    ")\n",
    "\n",
    "toxicity_score = toxicity_result[\"Toxicity_score\"][0]\n",
    "print(f'Toxicity: {toxicity_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Hallucinations\n",
    "\n",
    "Finally, let's detect potential hallucinations by comparing the generated response to the retrieved context used to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness to Context: 0.2740519344806671\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>Faithfulness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What year was the author born?</td>\n",
       "      <td>The author was born in 1964.</td>\n",
       "      <td>If he even knew about the strange classes I wa...</td>\n",
       "      <td>0.274052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            input                        output  \\\n",
       "0  What year was the author born?  The author was born in 1964.   \n",
       "\n",
       "                                        ground_truth  Faithfulness_score  \n",
       "0  If he even knew about the strange classes I wa...            0.274052  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What year was the author born?\"\n",
    "llm_response = query_engine.query(query)\n",
    "\n",
    "hallucination_result = eval.evaluate_data(\n",
    "    data=pd.DataFrame({\n",
    "        \"input\": [query], \n",
    "        \"output\": [llm_response.response],\n",
    "        \"ground_truth\": [llm_response.source_nodes[0].text]\n",
    "    }),\n",
    "    metrics=[Metric(name=\"Faithfulness\")]\n",
    ")\n",
    "\n",
    "print(f'Faithfulness to Context: {hallucination_result[\"Faithfulness_score\"][0]}')\n",
    "hallucination_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating AutoEval's Builtin Metrics\n",
    "\n",
    "LastMile's Builtin Metrics are a set of pre-defined metrics that cover a range of common evaluation tasks.\n",
    "- Faithfulness: Measures how closely the generated response matches the ground truth.\n",
    "- Relevance: Measures how relevant the generated response is to the input query.\n",
    "- Toxicity: Measures the toxicity of a generated response.\n",
    "- Answer Correctness: Measures how correct the generated response is.\n",
    "- Summarization: Measures how well the generated response summarizes the input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>Faithfulness_score</th>\n",
       "      <th>Relevance_score</th>\n",
       "      <th>Toxicity_score</th>\n",
       "      <th>Answer Correctness_score</th>\n",
       "      <th>Summarization_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where did the author grow up?</td>\n",
       "      <td>The author grew up in Yorkville, a neighborhoo...</td>\n",
       "      <td>England</td>\n",
       "      <td>0.058303</td>\n",
       "      <td>0.754154</td>\n",
       "      <td>0.058303</td>\n",
       "      <td>0.058303</td>\n",
       "      <td>0.058303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           input  \\\n",
       "0  Where did the author grow up?   \n",
       "\n",
       "                                              output ground_truth  \\\n",
       "0  The author grew up in Yorkville, a neighborhoo...      England   \n",
       "\n",
       "   Faithfulness_score  Relevance_score  Toxicity_score  \\\n",
       "0            0.058303         0.754154        0.058303   \n",
       "\n",
       "   Answer Correctness_score  Summarization_score  \n",
       "0                  0.058303             0.058303  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate\n",
    "from lastmile.lib.auto_eval import BuiltinMetrics\n",
    "\n",
    "query = \"Where did the author grow up?\"\n",
    "expected_response = \"England\"\n",
    "llm_response = query_engine.query(query)\n",
    "\n",
    "eval_result = eval.evaluate_data(\n",
    "    data=pd.DataFrame({\n",
    "    \"input\": [query],\n",
    "    \"output\": [llm_response.response],\n",
    "        \"ground_truth\": [expected_response]\n",
    "    }),\n",
    "    metrics=[BuiltinMetrics.FAITHFULNESS, BuiltinMetrics.RELEVANCE, BuiltinMetrics.TOXICITY, BuiltinMetrics.ANSWER_CORRECTNESS, BuiltinMetrics.SUMMARIZATION]\n",
    ")\n",
    "\n",
    "print(f'Evaluation results:')\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune a metric\n",
    "\n",
    "Follow the [Getting Started](https://github.com/lastmile-ai/lastmile-docs/blob/main/cookbook/AutoEval_Getting_Started.ipynb) guide to build a custom evaluation metric."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
