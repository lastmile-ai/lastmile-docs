{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a Retrieval-Augmented Generation (RAG) System\n",
    "\n",
    "This notebook provides a quick start guide on evaluating RAG systems using the Lastmile AutoEval SDK. We'll cover how to:\n",
    "\n",
    "1. Measure the faithfulness of a generated output against a ground truth and input\n",
    "2. Check the toxicity of an input and deny requests that are too toxic \n",
    "3. Detect hallucinations by comparing generated output to retrieved context\n",
    "\n",
    "Note: you will need an OpenAI API key set in your environment variables to run this notebook.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index>=0.11.0\n",
      "  Downloading llama_index-0.11.23-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index>=0.11.0)\n",
      "  Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index>=0.11.0)\n",
      "  Downloading llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.12.0,>=0.11.23 (from llama-index>=0.11.0)\n",
      "  Downloading llama_index_core-0.11.23-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index>=0.11.0)\n",
      "  Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index>=0.11.0)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index>=0.11.0)\n",
      "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.3.0,>=0.2.10 (from llama-index>=0.11.0)\n",
      "  Downloading llama_index_llms_openai-0.2.16-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index>=0.11.0)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl.metadata (729 bytes)\n",
      "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index>=0.11.0)\n",
      "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index>=0.11.0)\n",
      "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.4.0,>=0.3.0 (from llama-index>=0.11.0)\n",
      "  Downloading llama_index_readers_file-0.3.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index>=0.11.0)\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index>=0.11.0)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index>=0.11.0)\n",
      "  Downloading openai-1.54.4-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading SQLAlchemy-2.0.36-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.7 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading aiohttp-3.11.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting httpx (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting numpy<2.0.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading pillow-11.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting requests>=2.31.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading tiktoken-0.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions>=4.5.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index>=0.11.0)\n",
      "  Downloading llama_cloud-0.1.5-py3-none-any.whl.metadata (763 bytes)\n",
      "INFO: pip is looking at multiple versions of llama-index-indices-managed-llama-cloud to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index>=0.11.0)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pandas (from llama-index-legacy<0.10.0,>=0.9.48->llama-index>=0.11.0)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index>=0.11.0)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index>=0.11.0)\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index>=0.11.0)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "INFO: pip is looking at multiple versions of llama-index-readers-llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index>=0.11.0)\n",
      "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index>=0.11.0)\n",
      "  Downloading llama_parse-0.5.14-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting click (from nltk>3.8.1->llama-index>=0.11.0)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index>=0.11.0)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>3.8.1->llama-index>=0.11.0)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached propcache-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading yarl-1.17.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (66 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.4.0,>=0.3.0->llama-index>=0.11.0)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting anyio (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sniffio (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index>=0.11.0)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index>=0.11.0)\n",
      "  Downloading jiter-0.7.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached pydantic_core-2.23.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached charset_normalizer-3.4.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0)\n",
      "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index>=0.11.0) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index>=0.11.0)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index>=0.11.0)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.23->llama-index>=0.11.0) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index>=0.11.0) (1.16.0)\n",
      "Downloading llama_index-0.11.23-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_core-0.11.23-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.6.0-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai-0.2.16-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.3.0-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.2-cp312-cp312-macosx_11_0_arm64.whl (454 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading llama_cloud-0.1.5-py3-none-any.whl (188 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading llama_parse-0.5.14-py3-none-any.whl (13 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.54.4-py3-none-any.whl (389 kB)\n",
      "Downloading pillow-11.0.0-cp312-cp312-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading SQLAlchemy-2.0.36-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.8.0-cp312-cp312-macosx_11_0_arm64.whl (982 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m982.6/982.6 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Downloading greenlet-3.1.1-cp312-cp312-macosx_11_0_universal2.whl (274 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading jiter-0.7.1-cp312-cp312-macosx_11_0_arm64.whl (304 kB)\n",
      "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached propcache-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (45 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading yarl-1.17.2-cp312-cp312-macosx_11_0_arm64.whl (92 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: striprtf, pytz, filetype, dirtyjson, wrapt, urllib3, tzdata, typing-extensions, tqdm, tenacity, soupsieve, sniffio, regex, PyYAML, pypdf, propcache, pillow, numpy, networkx, mypy-extensions, multidict, marshmallow, joblib, jiter, idna, h11, greenlet, fsspec, frozenlist, distro, click, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, pydantic-core, pandas, nltk, httpcore, deprecated, beautifulsoup4, anyio, aiosignal, tiktoken, pydantic, httpx, dataclasses-json, aiohttp, openai, llama-index-core, llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.3 aiohttp-3.11.2 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.6.2.post1 attrs-24.2.0 beautifulsoup4-4.12.3 certifi-2024.8.30 charset-normalizer-3.4.0 click-8.1.7 dataclasses-json-0.6.7 deprecated-1.2.15 dirtyjson-1.0.8 distro-1.9.0 filetype-1.2.0 frozenlist-1.5.0 fsspec-2024.10.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.27.2 idna-3.10 jiter-0.7.1 joblib-1.4.2 llama-cloud-0.1.5 llama-index-0.11.23 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-core-0.11.23 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.6.0 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.2.16 llama-index-multi-modal-llms-openai-0.2.3 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.3.0 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.14 marshmallow-3.23.1 multidict-6.1.0 mypy-extensions-1.0.0 networkx-3.4.2 nltk-3.9.1 numpy-1.26.4 openai-1.54.4 pandas-2.2.3 pillow-11.0.0 propcache-0.2.0 pydantic-2.9.2 pydantic-core-2.23.4 pypdf-5.1.0 pytz-2024.2 regex-2024.11.6 requests-2.32.3 sniffio-1.3.1 soupsieve-2.6 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.8.0 tqdm-4.67.0 typing-extensions-4.12.2 typing-inspect-0.9.0 tzdata-2024.2 urllib3-2.2.3 wrapt-1.16.0 yarl-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting lastmile\n",
      "  Downloading lastmile-0.3.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from lastmile) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from lastmile) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from lastmile) (0.27.2)\n",
      "Requirement already satisfied: pandas in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from lastmile) (2.2.3)\n",
      "Collecting pandas-stubs (from lastmile)\n",
      "  Downloading pandas_stubs-2.2.3.241009-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from lastmile) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from lastmile) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from lastmile) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->lastmile) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->lastmile) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->lastmile) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->lastmile) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->lastmile) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->lastmile) (2.23.4)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from pandas->lastmile) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from pandas->lastmile) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from pandas->lastmile) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from pandas->lastmile) (2024.2)\n",
      "Collecting types-pytz>=2022.1.1 (from pandas-stubs->lastmile)\n",
      "  Downloading types_pytz-2024.2.0.20241003-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->lastmile) (1.16.0)\n",
      "Downloading lastmile-0.3.0-py3-none-any.whl (109 kB)\n",
      "Downloading pandas_stubs-2.2.3.241009-py3-none-any.whl (157 kB)\n",
      "Downloading types_pytz-2024.2.0.20241003-py3-none-any.whl (5.2 kB)\n",
      "Installing collected packages: types-pytz, pandas-stubs, lastmile\n",
      "Successfully installed lastmile-0.3.0 pandas-stubs-2.2.3.241009 types-pytz-2024.2.0.20241003\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saqadri/lm/lastmile-docs/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Dependencies\n",
    "%pip install \"llama-index>=0.11.0\"\n",
    "%pip install lastmile\n",
    "%pip install pandas\n",
    "\n",
    "# Setup Pandas to display without truncation (for display purposes)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LlamaIndex RAG System\n",
    "\n",
    "First, let's setup a simple RAG system using LlamaIndex to generate responses. \n",
    "Note: llama index defaults to using OpenAI as the LLM, But feel free to swap it out with your preferred LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data/PaulGrahamEssay\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating With Base Metric Faithfulness \n",
    "\n",
    "Now let's evaluate how faithful the generated response is to the ground truth, given an input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evlauation results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>Faithfulness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where did the author grow up?</td>\n",
       "      <td>The author grew up in Yorkville, a neighborhoo...</td>\n",
       "      <td>England</td>\n",
       "      <td>0.058303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           input  \\\n",
       "0  Where did the author grow up?   \n",
       "\n",
       "                                              output ground_truth  \\\n",
       "0  The author grew up in Yorkville, a neighborhoo...      England   \n",
       "\n",
       "   Faithfulness_score  \n",
       "0            0.058303  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lastmile.lib.auto_eval import AutoEval, Metric\n",
    "import pandas as pd\n",
    "\n",
    "eval = AutoEval()\n",
    "\n",
    "query = \"Where did the author grow up?\"\n",
    "expected_response = \"England\"\n",
    "llm_response = query_engine.query(query)\n",
    "\n",
    "eval_result = eval.evaluate_data(\n",
    "    data=pd.DataFrame({\n",
    "        \"input\": [query],\n",
    "        \"output\": [llm_response.response],\n",
    "        \"ground_truth\": [expected_response]\n",
    "    }),\n",
    "    metrics=[Metric(name=\"Faithfulness\")]\n",
    ")\n",
    "\n",
    "print(f'Evaluation results:')\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Output Toxicity\n",
    "\n",
    "We can also check the toxicity of the output and deny responses that are too toxic. Designed to detect and flag low-quality or potentially harmful AI-generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity: 0.0\n"
     ]
    }
   ],
   "source": [
    "toxicity_result = eval.evaluate_data(\n",
    "    data=pd.DataFrame({\"output\": [llm_response.response], \"input\": [\"\"], \"ground_truth\": [\"\"]}),\n",
    "    metrics=[Metric(name=\"Toxicity\")]\n",
    ")\n",
    "\n",
    "toxicity_score = toxicity_result[\"Toxicity_score\"][0]\n",
    "print(f'Toxicity: {toxicity_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Hallucinations\n",
    "\n",
    "Finally, let's detect potential hallucinations by comparing the generated response to the retrieved context used to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness to Context: 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>Faithfulness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What year was the author born?</td>\n",
       "      <td>The author was born in 1964.</td>\n",
       "      <td>If he even knew about the strange classes I wa...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            input                        output  \\\n",
       "0  What year was the author born?  The author was born in 1964.   \n",
       "\n",
       "                                        ground_truth  Faithfulness_score  \n",
       "0  If he even knew about the strange classes I wa...                 0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What year was the author born?\"\n",
    "llm_response = query_engine.query(query)\n",
    "\n",
    "hallucination_result = eval.evaluate_data(\n",
    "    data=pd.DataFrame({\n",
    "        \"input\": [query], \n",
    "        \"output\": [llm_response.response],\n",
    "        \"ground_truth\": [llm_response.source_nodes[0].text]\n",
    "    }),\n",
    "    metrics=[Metric(name=\"Faithfulness\")]\n",
    ")\n",
    "\n",
    "print(f'Faithfulness to Context: {hallucination_result[\"Faithfulness_score\"][0]}')\n",
    "hallucination_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating AutoEval's Builtin Metrics\n",
    "\n",
    "LastMile's Builtin Metrics are a set of pre-defined metrics that cover a range of common evaluation tasks.\n",
    "- Faithfulness: Measures how closely the generated response matches the ground truth.\n",
    "- Relevance: Measures how relevant the generated response is to the input query.\n",
    "- Toxicity: Measures the toxicity of a generated response.\n",
    "- Answer Correctness: Measures how correct the generated response is.\n",
    "- Summarization: Measures how well the generated response summarizes the input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>Faithfulness_score</th>\n",
       "      <th>Relevance_score</th>\n",
       "      <th>Toxicity_score</th>\n",
       "      <th>Answer Correctness_score</th>\n",
       "      <th>Summarization_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where did the author grow up?</td>\n",
       "      <td>The author grew up in England.</td>\n",
       "      <td>England</td>\n",
       "      <td>0.998955</td>\n",
       "      <td>0.948637</td>\n",
       "      <td>0.998955</td>\n",
       "      <td>0.998955</td>\n",
       "      <td>0.998955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           input                          output ground_truth  \\\n",
       "0  Where did the author grow up?  The author grew up in England.      England   \n",
       "\n",
       "   Faithfulness_score  Relevance_score  Toxicity_score  \\\n",
       "0            0.998955         0.948637        0.998955   \n",
       "\n",
       "   Answer Correctness_score  Summarization_score  \n",
       "0                  0.998955             0.998955  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate\n",
    "from lastmile.lib.auto_eval import BuiltinMetrics\n",
    "\n",
    "query = \"Where did the author grow up?\"\n",
    "expected_response = \"England\"\n",
    "llm_response = query_engine.query(query)\n",
    "\n",
    "eval_result = eval.evaluate_data(\n",
    "    data=pd.DataFrame({\n",
    "    \"input\": [query],\n",
    "    \"output\": [llm_response.response],\n",
    "        \"ground_truth\": [expected_response]\n",
    "    }),\n",
    "    metrics=[BuiltinMetrics.FAITHFULNESS, BuiltinMetrics.RELEVANCE, BuiltinMetrics.TOXICITY, BuiltinMetrics.ANSWER_CORRECTNESS, BuiltinMetrics.SUMMARIZATION]\n",
    ")\n",
    "\n",
    "print(f'Evaluation results:')\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune a metric\n",
    "\n",
    "Follow the [Getting Started](https://github.com/lastmile-ai/lastmile-docs/blob/main/cookbook/AutoEval_Getting_Started.ipynb) guide to build a custom evaluation metric."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
