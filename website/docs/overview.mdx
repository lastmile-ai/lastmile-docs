---
id: overview
slug: /
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import constants from "@site/core/tabConstants";

import { Card } from "@site/src/components/Card";
import { Grid } from "@site/src/components/Grid";
import { SplitPane } from "@site/src/components/SplitPane";
import { GettingStartedCard } from "@site/src/components/Home/GettingStartedCard";

# Introduction

LastMile is the [full-stack developer platform](/autoeval) to debug, evaluate and <u>improve</u> LLM applications. We make it easy to fine-tune custom evaluators, set up guardrails & monitor app performance.

<GettingStartedCard
  defaultLanguage="python"
  codeBlocks={[
    {
      language: "python",
      code: `from lastmile.lib.auto_eval import AutoEval, Metric
import pandas as pd

result = AutoEval().evaluate_data(
    data=pd.DataFrame({
        "input": ["Where did the author grow up?"],
        "output": ["France"],
        "ground_truth": ["England"]
    }),
    metrics=[Metric(name="Faithfulness")]
)

print(f'Evlauation result:', result)`,
    },
    {
      language: "javascript",
      label: "node.js",
      code: `import {Lastmile, Metric} from 'lastmile';

const client = new Lastmile();

const response = await client.evaluation.evaluate({
    input: ["Where did the author grow up?"],
    output: ["France"],
    groundTruth: ["England"]
    metric: Metric(name: "Faithfulness")
  });
`,
},
]}
/>

## Design your own metric

Use the fine-tuning service to design your own evaluators that represent custom criteria for your app quality.

<Grid className="custom-grid customize">
  <Card
    href="/autoeval/datasets"
    title="1. Create Datasets"
    description="Upload and manage application data for running and training evals, and generate synthetic labels."
    className="custom-card datasets"
  />
  <Card
    href="/autoeval/labeling"
    title="2. LLM Judge Active Labeling"
    description="Generate high-quality labels for your data using LLM Judge with human-in-the-loop"
    className="custom-card labeling"
  />
  <Card
    href="/autoeval/fine-tune"
    title="3. Fine-tune Models"
    description="Use the AutoEval fine-tuning service to develop custom metrics for your application."
    className="custom-card fine-tune"
  />
  <Card
    href="/autoeval/models"
    title="4. Run Evals"
    description="Compute metrics by running high-performance inference on a prebuilt or fine-tuned model."
    className="custom-card models"
  />
</Grid>

## Out-of-the-box metrics

Batteries-included evaluation metrics covering common AI application types, such as RAG and multi-agent compound AI systems.

<Grid className="custom-grid">
  <Card
    href="/autoeval/metrics"
    title="Faithfulness"
    description="Measures how adherent or faithful an LLM response is to the provided context. Often used for hallucination detection."
    className="custom-card faithfulness"
  />
  <Card
    href="/autoeval/metrics"
    title="Semantic Similarity"
    description="Measures semantic similarity between two strings. Often used for context relevance, or input/output relevance, or similarity between a response and ground truth."
    className="custom-card similarity"
  />
  <Card
    href="/autoeval/metrics"
    title="Summarization Quality"
    description="Quantify the quality of a summarization response."
    className="custom-card summarization"
  />
  <Card
    href="/autoeval/metrics"
    title="Toxicity"
    description="Quantify the toxicity level in an LLM response."
    className="custom-card toxicity"
  />
  <Card
    href="/autoeval/metrics"
    title="More"
    description="Explore other metrics available in AutoEval, or keep reading to design your own metric."
    className="custom-card"
  />
</Grid>

## Meet alBERTa üçÅ

alBERTa is a family of small language models (SLMs) designed for evaluation. They are optimized to be:

- **small** -- 400M parameter entailment model
- **fast** -- can run inference on CPU in < 300ms
- **customizable** -- fine-tune for custom evaluation tasks

<Grid className="alberta-grid">
  <Card
    href="/autoeval/models"
    title="alBERTa-512 üçÅ"
    description="2048 token context, specialized for evaluation tasks (like faithfulness), and gives a numeric 0->1 score."
    backgroundColor="#F5F5F5"
    className="custom-card model-512"
  />
  <Card
    href="/autoeval/models"
    title="alBERTa-LC-8k üçÅ"
    description="Long-context window variant that can scale to 128k+ tokens using a scaled dot-product attention layer"
    backgroundColor="#F5F5F5"
    className="custom-card model-8k"
  />
</Grid>

## Explore our guides

<Grid className="guides-grid">
  <Card
    href="https://github.com/lastmile-ai/lastmile-docs/blob/main/cookbook/AutoEval_Getting_Started.ipynb"
    title="Quickstart"
    description="Start-to-finish overview of AutoEval, from running evals, labeling with LLM Judge to fine-tuning a custom metric."
    className="custom-card"
  />
  <Card
    href="https://github.com/lastmile-ai/lastmile-docs/blob/main/cookbook/RAG_Evaluation.ipynb"
    title="Retrieval systems"
    description="Evaluate a RAG application for hallucination, relevance and other out-of-the-box metrics available via AutoEval."
    className="custom-card"
  />
  <!-- <Card
    href="/guides/multi-agent-evaluation"
    title="Multi-agent applications"
    description="Evaluate end-to-end and intermediate step metrics for a compound AI system."
    className="custom-card"
  /> -->
  <Card
    href="https://github.com/lastmile-ai/lastmile-docs/blob/main/cookbook/RAG_Guardrails.ipynb"
    title="Real-time guardrails"
    description="Build real-time guardrails in a RAG application using fine-tuned alBERTa üçÅ models."
    className="custom-card"
  />
</Grid>
