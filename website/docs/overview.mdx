---
id: overview
slug: /
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import constants from "@site/core/tabConstants";

import { Card } from "@site/src/components/Card";
import { Grid } from "@site/src/components/Grid";
import { SplitPane } from "@site/src/components/SplitPane";
import { GettingStartedCard } from "@site/src/components/Home/GettingStartedCard";

# Introduction

LastMile is the [full-stack developer platform](/autoeval) to debug, evaluate and <u>improve</u> LLM applications. We make it easy to [fine-tune](/autoeval/fine-tune) custom evaluators, set up [guardrails](/autoeval/guardrails) & monitor app performance.

<GettingStartedCard
  defaultLanguage="python"
  codeBlocks={[
    {
      language: "python",
      code: `from lastmile.lib.auto_eval import AutoEval, Metric
import pandas as pd

result = AutoEval().evaluate_data(
    data=pd.DataFrame({
        "input": ["Where did the author grow up?"],
        "output": ["France"],
        "ground_truth": ["England"]
    }),
    metrics=[Metric(name="Faithfulness")]
)

print(f'Evlauation result:', result)`,
    },
    {
      language: "javascript",
      label: "node.js",
      code: `import { AutoEval, Metric, BuiltinMetrics } from "lastmile/lib/auto_eval";

const client = new AutoEval();
const result = await client.evaluateData(
    /*data*/ [
      {
        input: "Where did the author grow up?",
        output: "France",
        ground_truth: "England",
      },
    ],
    /*metrics*/ [BuiltinMetrics.FAITHFULNESS]
);

console.table(result);
`,
},
]}
/>

## Design your own metric

Use the fine-tuning service to design your own evaluators that represent custom criteria for your app quality.

<Grid className="custom-grid customize">
  <Card
    href="/autoeval/datasets"
    title="1. Create Datasets"
    description="Upload and manage application data for running and training evals, and generate synthetic labels."
    className="custom-card datasets"
  />
  <Card
    href="/autoeval/labeling"
    title="2. Synthetic Labeling"
    description="Generate high-quality labels for your data using LLM Judge with human-in-the-loop to refine synthetic labels."
    className="custom-card labeling"
  />
  <Card
    href="/autoeval/fine-tune"
    title="3. Fine-tune Evaluators"
    description="Use the AutoEval fine-tuning service to develop custom metrics for your application."
    className="custom-card fine-tune"
  />
  <Card
    href="/autoeval/models#usage-guide"
    title="4. Run Evals"
    description="Compute metrics by running high-performance inference using a prebuilt or fine-tuned model."
    className="custom-card models"
  />
</Grid>

## Out-of-the-box metrics

Batteries-included evaluation metrics covering common AI application types, such as RAG and multi-agent [compound AI systems](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/).

<Grid className="custom-grid">
  <Card
    href="/autoeval/metrics#hallucination-detection-faithfulness"
    title="Faithfulness"
    description="Measures how adherent or faithful an LLM response is to the provided context. Often used for hallucination detection."
    className="custom-card faithfulness"
  />
  <Card
    href="/autoeval/metrics#relevance"
    title="Relevance"
    description="Measures semantic similarity between two strings. Often used for context relevance, or input/output relevance, or similarity between a response and ground truth."
    className="custom-card similarity"
  />
  <Card
    href="/autoeval/metrics#summarization-score"
    title="Summarization Quality"
    description="Quantify the quality of a summarization response."
    className="custom-card summarization"
  />
  <Card
    href="/autoeval/metrics"
    title="Toxicity"
    description="Quantify the toxicity level in an LLM response."
    className="custom-card toxicity"
  />
  <Card
    href="/autoeval/metrics"
    title="More"
    description="Explore other metrics available in AutoEval, or keep reading to design your own metric."
    className="custom-card"
  />
</Grid>

## Meet alBERTa üçÅ

alBERTa is a family of small language models (SLMs) designed for evaluation. They are optimized to be:

- **small** -- 400M parameter entailment model
- **fast** -- can run inference on CPU in < 300ms
- **customizable** -- fine-tune for custom evaluation tasks

<Grid className="alberta-grid">
  <Card
    href="/autoeval/models#alberta-"
    title="alBERTa-512 üçÅ"
    description="512 token context, specialized for evaluation tasks (like faithfulness), and gives a numeric 0->1 score."
    backgroundColor="#F5F5F5"
    className="custom-card model-512"
  />
  <Card
    href="/autoeval/models#alberta-"
    title="alBERTa-LC-8k üçÅ"
    description="Long-context window variant that can scale up to 128k tokens using a scaled dot-product attention layer"
    backgroundColor="#F5F5F5"
    className="custom-card model-8k"
  />
</Grid>

## Explore our guides

<Grid className="guides-grid">
  <Card
    href="https://github.com/lastmile-ai/lastmile-docs/blob/main/cookbook/AutoEval_Getting_Started.ipynb"
    title="Quickstart"
    description="Start-to-finish overview of AutoEval, from running evals, labeling with LLM Judge to fine-tuning a custom metric."
    className="custom-card"
  />
  <Card
    href="https://github.com/lastmile-ai/lastmile-docs/blob/main/cookbook/RAG_Evaluation.ipynb"
    title="Retrieval systems"
    description="Evaluate a RAG application for hallucination, relevance and other out-of-the-box metrics available via AutoEval."
    className="custom-card"
  />
  <!-- <Card
    href="/guides/multi-agent-evaluation"
    title="Multi-agent applications"
    description="Evaluate end-to-end and intermediate step metrics for a compound AI system."
    className="custom-card"
  /> -->
  <Card
    href="https://github.com/lastmile-ai/lastmile-docs/blob/main/cookbook/RAG_Guardrails.ipynb"
    title="Real-time guardrails"
    description="Build real-time guardrails in a RAG application using fine-tuned alBERTa üçÅ models."
    className="custom-card"
  />
</Grid>
