# Experimentation Guide

Build and manage your **evaluation experiments** with **LastMile AI AutoEval**. Use the **AutoEval library** to create **Experiments**, a structured way to organize and track evaluation runs as you make iterative changes to your **AI application**.

Experiments allow you to **systematically test** the impact of changes, such as:
- Updating the **LLM model**
- Modifying the **retrieval strategy** for a **RAG system**
- Adjusting **system prompts** for an agent  
- And more  

### Usage Guide
This guide walks through the process of setting up and running experiments using AutoEval, including:
1. **Setting up the API key** and **creating a project**  
2. **Preparing and uploading a dataset**  
3. **Creating an Experiment**  
4. **Evaluating the dataset** against default metrics, logging results, and iterating on changes  
5. **Visualizing the results** in the Experiments Console  

---

## 1. Set Up AutoEval Client  

Before running experiments, ensure you have the latest version of AutoEval:

```bash
pip install lastmile --upgrade
```

#### Authenticate with the LastMile AI API  

To interact with the **LastMile AI API**, set your API key as an environment variable.

üìå **Tip:** If you don't have an API key yet, visit the **LastMile AI dashboard**, navigate to the **API section** on the sidebar, and copy your key.

```python
import os

api_token = "YOUR_API_KEY_HERE"

if not api_token:
    print("Error: Please set your API key in the environment variable LASTMILE_API_KEY")
elif api_token == "YOUR_API_KEY_HERE":
    print("Error: Please replace 'YOUR_API_KEY_HERE' with your actual API key")
else:
    print("‚úì API key successfully configured!")
```

### Set Up AutoEval Client  
Once authenticated, initialize the **AutoEval client**:

```python
# Setup Pandas to display without truncation (for display purposes)
import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', None)

from lastmile.lib.auto_eval import AutoEval

client = AutoEval(api_token=api_token)  # Optionally set project_id to scope to a specific project
```




---

## 2. Create a Project or Select an Existing Project  

A **Project** is the container that organizes your **Experiments, Evaluation runs, and Datasets**. It typically corresponds to the **AI initiative or application** you‚Äôre building.

Projects help keep evaluations structured, especially when managing multiple experiments across different AI models or applications. You can create new projects or use existing ones.

To create a new project programmatically, use:

```python
project = client.create_project(
    name="AutoEval Experiments", 
    description="Project to test AutoEval Experiments"
)

# Important - set the project_id in the client so all requests are scoped to this project
client.project_id = project.id
```

Once a project is created, you can list all available projects, including the default **"AutoEval"** project:

```python
# List all projects in your account
projects = client.list_projects()
projects
```

If you already have a project and want to use it, retrieve it using the `project_id`:

```python
default_project = client.get_project(project_id="z8kfriq6cga6j0fx38znw4y6")
default_project
```

‚úÖ **Next Step:** **Prepare and upload a dataset.**  

---

## 3. Prepare and Upload Your Dataset  

Now that the API key is configured, it's time to **prepare and upload a dataset** for evaluation.

LastMile AI AutoEval expects a **CSV file** with the following columns:
- **`input`**: The user's query or input text  
- **`output`**: The assistant's response to the user's query  
- **`ground_truth`** *(optional)*: The correct or expected response for comparison  

Uploading this dataset allows you to evaluate how well the assistant's responses align with the **ground truth** using LastMile AI‚Äôs evaluation metrics.

To upload your dataset, use the following code:

```python
dataset_csv_path = "ADD_YOUR_DATASET_HERE"

dataset_id = client.upload_dataset(
    file_path=dataset_csv_path,
    name="NAME_OF_YOUR_DATASET",
    description="DESCRIPTION_OF_DATASET"
)

print(f"Dataset created with ID: {dataset_id}")
```

‚úÖ **Next Step:** **Create an Experiment.**  

---

## 4. Create an Experiment  

To create an experiment, use the following code:

```python
experiment = client.create_experiment(
    name="EXPERIMENT_NAME",
    description="EXPERIMENT_DESCRIPTION",
    metadata={
        "model": "gpt-4o", 
        "temperature": 0.8, 
        "misc": {
            "dataset_version": "0.1.1", 
            "app": "customer-support"
        }
    }
)
```

To retrieve an experiment by ID:

```python
experiment = client.get_experiment(experiment_id=experiment.id)
```

‚úÖ **Next Step:** **Evaluate the dataset against default metrics.**  

---

## 5. Evaluate the Dataset Against Built-in Metrics  

```python
from lastmile.lib.auto_eval import BuiltinMetrics

default_metrics = [
    BuiltinMetrics.FAITHFULNESS,
    BuiltinMetrics.RELEVANCE,
]

print("Evaluation job kicked off")

evaluation_results = client.evaluate_dataset(
    dataset_id=dataset_id,
    metrics=default_metrics,
    experiment_id=experiment.id,
    metadata={"extras": "Base metric tests"}
)

print("Evaluation Results:")
evaluation_results.head(10)
```

‚úÖ **Next Step:** **Visualize in the Experiments Console.**  

---

## 6. Visualize in the Experiments Console  

üìä **Explore your results in the AutoEval UI:**
- üî¨ **Experiments Overview:** [View all experiments](https://lastmileai.dev/evaluations?view=experiments)
- üìà **Evaluation Runs:** [See all evaluation runs](https://lastmileai.dev/evaluations?view=all_runs)
- üè¢ **Project Dashboard:** [Manage projects and experiments](https://lastmileai.dev/dashboard)
- üìÇ **Dataset Library:** [Browse and manage uploaded datasets](https://lastmileai.dev/datasets)

üöÄ **Start iterating on your AI application based on the evaluation insights!**

