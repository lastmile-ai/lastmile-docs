"use strict";(self.webpackChunklastmile_docs=self.webpackChunklastmile_docs||[]).push([[9777],{57811:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>d,toc:()=>h});var n=a(74848),i=a(28453),l=a(4865),s=a(19365);a(79402);const o={title:"Synthetic Labeling"},r="Synthetic Labeling",d={id:"autoeval/labeling",title:"Synthetic Labeling",description:"Generate high-quality labels using LLMs + human refinement",source:"@site/docs/autoeval/labeling.mdx",sourceDirName:"autoeval",slug:"/autoeval/labeling",permalink:"/autoeval/labeling",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{title:"Synthetic Labeling"},sidebar:"sidebar",previous:{title:"Example Datasets",permalink:"/autoeval/datasets/example-datasets"},next:{title:"Fine-tune Evaluators",permalink:"/autoeval/fine-tune"}},c={},h=[{value:"Why?",id:"why",level:3},{value:"Usage Guide",id:"usage-guide",level:3},{value:"API",id:"api",level:4},{value:"UI",id:"ui",level:4}];function u(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"synthetic-labeling",children:"Synthetic Labeling"})}),"\n",(0,n.jsx)(t.p,{children:"Generate high-quality labels using LLMs + human refinement"}),"\n",(0,n.jsxs)(t.p,{children:["LLM Judge uses LLM\u2019s to evaluate the performance of your AI application. This is a good way to generate synthetic labeled datasets which can be distilled into a ",(0,n.jsx)(t.a,{href:"/autoeval/fine-tune",children:"fine-tuned evaluator model"}),"."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"AutoEval Flow",src:a(94832).A+"",width:"1748",height:"300"})}),"\n",(0,n.jsx)("u",{children:"Overall flow:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Upload"})," data: Start by ",(0,n.jsx)(t.a,{href:"/autoeval/datasets",children:"uploading"})," a few examples of the application (e.g. input/output/context)."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Specify eval criteria"})," (prompt): Define an evaluation criteria as a prompt, which is used by an LLM judge to label the app data."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Refine"})," with human-in-the-loop: A few examples are labeled by a human (active labeling) to iterate on the LLM judge labels."]}),"\n"]}),"\n",(0,n.jsx)(t.h3,{id:"why",children:"Why?"}),"\n",(0,n.jsx)(t.p,{children:"Most applications have few to no labels or ground truth data. For those use cases, AutoEval provides the ability to synthetically generate labels."}),"\n",(0,n.jsx)(t.p,{children:"The platform asks for an evaluation criteria (a prompt), and labels AI interactions as positive (1) or negative (0) using an LLM (LLM Judge).\nFrom there, a developer can manually refine some labels, which are fed back in to the labeling job as few-shot examples."}),"\n",(0,n.jsx)(t.h3,{id:"usage-guide",children:"Usage Guide"}),"\n",(0,n.jsx)(t.h4,{id:"api",children:"API"}),"\n",(0,n.jsx)(t.admonition,{type:"info",children:(0,n.jsxs)(t.p,{children:["See the ",(0,n.jsx)(t.a,{href:"/sdk",children:"API section"})," for more info on the API, such as provisioning API keys, examples, etc."]})}),"\n",(0,n.jsxs)(l.A,{groupId:"label_dataset",children:[(0,n.jsx)(s.default,{value:"python",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",metastring:'title="label_dataset"',children:'from lastmile.lib.auto_eval import AutoEval, BuiltinMetrics\n\nclient = AutoEval(api_token="api_token_if_LASTMILE_API_TOKEN_not_set")\n\njob_id = client.label_dataset(\n  dataset_id=dataset_id,\n  prompt_template=BuiltinMetrics.FAITHFULNESS, # Or a custom evaluation prompt criteria\n  wait_for_completion=False\n)\nprint("Waiting for job to complete...")\nclient.wait_for_label_dataset_job(job_id)\n\nlabeled_dataset = client.download_dataset(dataset_id)\n\nprint(f"Labeling Job with ID: {job_id} Completed")\n'})})}),(0,n.jsx)(s.default,{value:"node.js",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-typescript",metastring:'title="label_dataset"',children:'import { AutoEval, BuiltinMetrics } from "lastmile/lib/auto_eval";\n\nconst client = new AutoEval({\napiKey: "api_token_if_LASTMILE_API_TOKEN_not_set",\n});\n\nconst jobId = await client.labelDataset({\n  datasetId,\n  promptTemplate: BuiltinMetrics.FAITHFULNESS, // Or a custom evaluation prompt criteria\n  waitForCompletion: false, // Set to true to wait for the job to complete\n});\n\nconsole.log(`Waiting for labeling job ${jobId} to complete...`);\nawait client.waitForLabelDatasetJob(jobId);\nconsole.log(`Labeling Job with ID: ${jobId} Completed`);\nconst labeledData = client.downloadDataset(datasetId);\n\nconsole.table(labeledData);\n'})})})]}),"\n",(0,n.jsx)(t.h4,{id:"ui",children:"UI"}),"\n",(0,n.jsxs)(t.ol,{children:["\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:["Navigate to ",(0,n.jsx)(t.a,{href:"https://lastmileai.dev/datasets",children:"Dataset Library"})," and open a Dataset to label.\n",(0,n.jsx)(t.img,{alt:"View Dataset",src:a(52776).A+"",width:"2682",height:"1772"})]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:["Click ",(0,n.jsx)(t.strong,{children:"Generate LLM Judge Labels"}),"."]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"Specify evaluation criteria in the form of a prompt, or use one of the predefined templates."}),"\n",(0,n.jsx)(t.admonition,{type:"tip",children:(0,n.jsxs)(t.p,{children:["You can reference column names inside the prompt using ",(0,n.jsx)(t.code,{children:"{column_name}"})," syntax. For example, if your evaluation criteria needs to reference the output, add ",(0,n.jsx)(t.code,{children:"{output}"})," to the prompt string"]})}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"(optional) Provide few-shot examples to be used to refine the quality of the labels you are generating."}),"\n",(0,n.jsx)(t.admonition,{type:"tip",children:(0,n.jsx)(t.p,{children:"We recommend having a diverse set of examples for your 5-20 examples for optimal performance."})}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:["Click ",(0,n.jsx)(t.strong,{children:"Start Labeling"})," and we'll generate 16 labels for you to review and edit\n",(0,n.jsx)(t.img,{alt:"Start Labeling",src:a(10650).A+"",width:"2684",height:"1766"})]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:["Review the labels to see if the labels accurately represent your evaluation criteria.\nIf not, you can manually update the label, or ",(0,n.jsx)(t.strong,{children:"Change Configuration"})," to update the prompt.\n",(0,n.jsx)(t.img,{alt:"Review Labels",src:a(5685).A+"",width:"2680",height:"1758"})]}),"\n",(0,n.jsx)(t.admonition,{type:"tip",children:(0,n.jsx)(t.p,{children:"If you don't want to review all 16 labels, click on the last page dot directly."})}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:["Click ",(0,n.jsx)(t.strong,{children:"Review and Submit"}),", which shows all 16 labels in one table, and gives you a final change to update them.\nThese labels are used as further few-shot prompts to help improve the LLM Judge labeling quality."]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:["Click ",(0,n.jsx)(t.strong,{children:"Submit"})," to start the full labeling job"]}),"\n"]}),"\n"]})]})}function p(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(u,{...e})}):u(e)}},79402:(e,t,a)=>{var n=a(38193);!!n.default.canUseDOM&&navigator.platform.startsWith("Mac"),!!n.default.canUseDOM&&navigator.platform.startsWith("Win")},4865:(e,t,a)=>{a.d(t,{A:()=>p});var n=a(96540),i=a(18215),l=a(23104),s=a(47751),o=a(92303);const r={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var d=a(74848);function c(e){let{className:t,block:a,selectedValue:n,selectValue:s,tabValues:o}=e;const c=[],{blockElementScrollPositionUntilNextRender:h}=(0,l.a_)(),u=e=>{const t=e.currentTarget,a=c.indexOf(t),i=o[a].value;i!==n&&(h(t),s(i))},p=e=>{let t=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const a=c.indexOf(e.currentTarget)+1;t=c[a]??c[0];break}case"ArrowLeft":{const a=c.indexOf(e.currentTarget)-1;t=c[a]??c[c.length-1];break}}t?.focus()};return(0,d.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":a},t),children:o.map((e=>{let{value:t,label:a,attributes:l}=e;return(0,d.jsx)("li",{role:"tab",tabIndex:n===t?0:-1,"aria-selected":n===t,ref:e=>c.push(e),onKeyDown:p,onClick:u,...l,className:(0,i.A)("tabs__item",r.tabItem,l?.className,{"tabs__item--active":n===t}),children:a??t},t)}))})}function h(e){let{lazy:t,children:a,selectedValue:l}=e;const s=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=s.find((e=>e.props.value===l));return e?(0,n.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,d.jsx)("div",{className:"margin-top--md",children:s.map(((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==l})))})}function u(e){const t=(0,s.u)(e);return(0,d.jsxs)("div",{className:(0,i.A)("tabs-container",r.tabList),children:[(0,d.jsx)(c,{...t,...e}),(0,d.jsx)(h,{...t,...e})]})}function p(e){const t=(0,o.default)();return(0,d.jsx)(u,{...e,children:(0,s.v)(e.children)},String(t))}},94832:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/autoeval_flow_labeling-9818c7a047f13db7f7e8e594b4b9351f.png"},52776:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/get_dataset-8935c26240fb99111edbe644d66d28be.png"},10650:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/labeling_criteria-45edd28e8332af5ddac864a1b1bcbe79.png"},5685:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/labeling_review-f3b5cc369284081e8b521b33937cfe11.png"}}]);