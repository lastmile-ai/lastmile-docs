"use strict";(self.webpackChunklastmile_docs=self.webpackChunklastmile_docs||[]).push([[5249],{15753:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>u});var a=n(74848),r=n(28453);n(4865),n(19365),n(79402);const i={id:"autoeval-intro",title:"Introduction"},s=void 0,o={id:"autoeval/autoeval-intro",title:"Introduction",description:"Testing is the most important step in both LLM application development and monitoring it's behavior in production. In Machine Learning and Artificial Intelligence, testing is referred to by different names depending on when and how you test. When testing an LLM application during development, the process is often referred to as Evaluation. When testing an LLM application's behavior during production (typically for an real-time/online use case), the testing is referred to as Guardrails. We'll cover both of these in more depth.",source:"@site/docs/autoeval/introduction.mdx",sourceDirName:"autoeval",slug:"/autoeval/autoeval-intro",permalink:"/autoeval/autoeval-intro",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"autoeval-intro",title:"Introduction"},sidebar:"sidebar",previous:{title:"AutoEval Developer Platform",permalink:"/autoeval/"},next:{title:"Evaluation Metrics",permalink:"/autoeval/metrics"}},l={},u=[{value:"Evaluation",id:"evaluation",level:2},{value:"Guardrails",id:"guardrails",level:2}];function c(e){const t={a:"a",code:"code",em:"em",h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(t.p,{children:["Testing is the most important step in both LLM application development and monitoring it's behavior in production. In Machine Learning and Artificial Intelligence, testing is referred to by different names depending on ",(0,a.jsx)(t.em,{children:"when"})," and ",(0,a.jsx)(t.em,{children:"how"})," you test. When testing an LLM application during development, the process is often referred to as ",(0,a.jsx)(t.strong,{children:"Evaluation"}),". When testing an LLM application's behavior during production (typically for an real-time/online use case), the testing is referred to as ",(0,a.jsx)(t.strong,{children:"Guardrails"}),". We'll cover both of these in more depth."]}),"\n",(0,a.jsx)(t.h2,{id:"evaluation",children:"Evaluation"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Evaluation"})," is the testing and assessment of how well the LLM is performing for the task it was designed to solve."]}),"\n",(0,a.jsxs)(t.p,{children:["For retrieval augmented generation (RAG) chatbots, developers will evaluate ",(0,a.jsx)(t.em,{children:"how well does the chatbot answer questions"}),". Given the wide array of capabilities for LLMs, the process of evaluating it's performance has become significantly more difficult than non-LLM models of the past. ",(0,a.jsx)(t.a,{href:"https://github.com/openai/simple-evals?tab=readme-ov-file#benchmark-results",children:"OpenAI releases benchmarks (evaluations)"})," for their models and they include measuring their model's performance for ",(0,a.jsx)(t.code,{children:"question answers"}),", ",(0,a.jsx)(t.code,{children:"math"}),", ",(0,a.jsx)(t.code,{children:"reasoning"}),", ",(0,a.jsx)(t.code,{children:"multitask language understanding"}),", etc."]}),"\n",(0,a.jsxs)(t.p,{children:["Two popular approaches to evaluate these LLM applications matching their flexibility with an equally flexible evalation is ",(0,a.jsx)(t.strong,{children:"human-in-the-loop"})," and ",(0,a.jsx)(t.strong,{children:"LLM-as-a-judge"}),". Human-in-the-loop relies on subject matter experts to label and verify whether the LLM application is correct. LLM-as-a-judge uses either the same LLM or another LLM to evaluate the performance of the application. Both approaches have their advantages (flexible) and shortcomings (cost and time)."]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"LastMile AI"})," combines the advantages of these approaches with a few other traditional ML techniques (",(0,a.jsx)(t.strong,{children:"active learning"}),", ",(0,a.jsx)(t.strong,{children:"synthetic data generation"}),", and ",(0,a.jsx)(t.strong,{children:"fine-tuning"}),") to provide the best-in-class evaluators."]}),"\n",(0,a.jsx)(t.h2,{id:"guardrails",children:"Guardrails"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Guardrails"})," is the testing and assessment of the quality of the LLM application's results in a live or production setting."]}),"\n",(0,a.jsxs)(t.p,{children:["A general rule of thumb is ",(0,a.jsx)(t.em,{children:"Everything is harder with live data and in production"}),". Guardrails act as the quality control for an LLM returning results in real-time."]}),"\n",(0,a.jsx)(t.p,{children:"Considerations for whether a guardrail can be used for an LLM application include:"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Latency"})," - can a guardrail give results in milliseconds without negatively impacting the user experience?"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Consistency"})," - is the guardrail dependable or will it give false positives or false negatives?"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Scalability"})," - can the guardrail scale for spikes in user traffic?"]}),"\n"]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"LastMile AI"})," provides the only low-latency fine-tuned guardrails that can be used for production LLM applications."]})]})}function d(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},79402:(e,t,n)=>{var a=n(38193);!!a.default.canUseDOM&&navigator.platform.startsWith("Mac"),!!a.default.canUseDOM&&navigator.platform.startsWith("Win")},19365:(e,t,n)=>{n.r(t),n.d(t,{default:()=>s});n(96540);var a=n(18215);const r={tabItem:"tabItem_Ymn6"};var i=n(74848);function s(e){let{children:t,hidden:n,className:s}=e;return(0,i.jsx)("div",{role:"tabpanel",className:(0,a.A)(r.tabItem,s),hidden:n,children:t})}},4865:(e,t,n)=>{n.d(t,{A:()=>p});var a=n(96540),r=n(18215),i=n(23104),s=n(47751),o=n(92303);const l={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var u=n(74848);function c(e){let{className:t,block:n,selectedValue:a,selectValue:s,tabValues:o}=e;const c=[],{blockElementScrollPositionUntilNextRender:d}=(0,i.a_)(),h=e=>{const t=e.currentTarget,n=c.indexOf(t),r=o[n].value;r!==a&&(d(t),s(r))},p=e=>{let t=null;switch(e.key){case"Enter":h(e);break;case"ArrowRight":{const n=c.indexOf(e.currentTarget)+1;t=c[n]??c[0];break}case"ArrowLeft":{const n=c.indexOf(e.currentTarget)-1;t=c[n]??c[c.length-1];break}}t?.focus()};return(0,u.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},t),children:o.map((e=>{let{value:t,label:n,attributes:i}=e;return(0,u.jsx)("li",{role:"tab",tabIndex:a===t?0:-1,"aria-selected":a===t,ref:e=>c.push(e),onKeyDown:p,onClick:h,...i,className:(0,r.A)("tabs__item",l.tabItem,i?.className,{"tabs__item--active":a===t}),children:n??t},t)}))})}function d(e){let{lazy:t,children:n,selectedValue:i}=e;const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=s.find((e=>e.props.value===i));return e?(0,a.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,u.jsx)("div",{className:"margin-top--md",children:s.map(((e,t)=>(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==i})))})}function h(e){const t=(0,s.u)(e);return(0,u.jsxs)("div",{className:(0,r.A)("tabs-container",l.tabList),children:[(0,u.jsx)(c,{...t,...e}),(0,u.jsx)(d,{...t,...e})]})}function p(e){const t=(0,o.default)();return(0,u.jsx)(h,{...e,children:(0,s.v)(e.children)},String(t))}},47751:(e,t,n)=>{n.d(t,{u:()=>p,v:()=>u});var a=n(96540),r=n(56347),i=n(205),s=n(57485),o=n(31682),l=n(70679);function u(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function c(e){const{values:t,children:n}=e;return(0,a.useMemo)((()=>{const e=t??function(e){return u(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:r}}=e;return{value:t,label:n,attributes:a,default:r}}))}(n);return function(e){const t=(0,o.XI)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function d(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function h(e){let{queryString:t=!1,groupId:n}=e;const i=(0,r.W6)(),o=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,s.aZ)(o),(0,a.useCallback)((e=>{if(!o)return;const t=new URLSearchParams(i.location.search);t.set(o,e),i.replace({...i.location,search:t.toString()})}),[o,i])]}function p(e){const{defaultValue:t,queryString:n=!1,groupId:r}=e,s=c(e),[o,u]=(0,a.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!d({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:s}))),[p,f]=h({queryString:n,groupId:r}),[m,v]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[r,i]=(0,l.Dv)(n);return[r,(0,a.useCallback)((e=>{n&&i.set(e)}),[n,i])]}({groupId:r}),g=(()=>{const e=p??m;return d({value:e,tabValues:s})?e:null})();(0,i.A)((()=>{g&&u(g)}),[g]);return{selectedValue:o,selectValue:(0,a.useCallback)((e=>{if(!d({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);u(e),f(e),v(e)}),[f,v,s]),tabValues:s}}},28453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>o});var a=n(96540);const r={},i=a.createContext(r);function s(e){const t=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),a.createElement(i.Provider,{value:t},e.children)}}}]);