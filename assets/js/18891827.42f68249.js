/*! For license information please see 18891827.42f68249.js.LICENSE.txt */
(self.webpackChunklastmile_docs=self.webpackChunklastmile_docs||[]).push([[1235],{69003:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>b,contentTitle:()=>x,default:()=>k,frontMatter:()=>v,metadata:()=>j,toc:()=>y});var s=a(74848),i=a(28453),r=a(4865),n=a(19365),l=(a(79402),a(96540),a(46942)),o=a.n(l);const c={card:"card_kyB9",cardTitle:"cardTitle_WmU7",icon:"icon_xMNd"};function u(e){let{href:t,title:a,description:i,className:r}=e;return(0,s.jsxs)("a",{href:t,className:o()(c.card,r),children:[(0,s.jsxs)("h3",{className:c.cardTitle,children:[a,(0,s.jsx)("span",{className:c.icon,children:">"})]}),(0,s.jsx)("p",{children:i})]})}const d={grid:"grid_qyhz"};function m(e){let{children:t,className:a}=e;return(0,s.jsx)("div",{className:o()(d.grid,a),children:t})}const h={splitPane:"splitPane_bO8i",leftPane:"leftPane_de69",rightPane:"rightPane_Z18N",icon:"icon_QiKa"};function f(e){let{leftChild:t,rightChild:a,className:i,leftPaneClassName:r,rightPaneClassName:n}=e;return(0,s.jsxs)("div",{className:o()(h.splitPane,i),children:[(0,s.jsx)("div",{className:o()(h.leftPane,r),children:t}),(0,s.jsx)("div",{className:o()(h.rightPane,n),children:a})]})}var p=a(52138);function g(e){let{codeBlocks:t,defaultLanguage:a}=e;return(0,s.jsx)(f,{className:"getting-started-card",leftPaneClassName:"getting-started-card-left-pane",rightPaneClassName:"getting-started-card-right-pane",leftChild:(0,s.jsxs)("a",{href:"/quickstart",children:[(0,s.jsx)("h3",{children:"Developer quickstart"}),(0,s.jsx)("p",{children:"Compute your first evaluation metric within 5 minutes."})]}),rightChild:(0,s.jsx)(r.A,{children:t.map((e=>{let{language:t,label:i,code:r}=e;return(0,s.jsx)(n.default,{value:i??t,label:i??t,default:a===t,children:(0,s.jsx)(p.default,{language:t,children:r})},i??t)}))})})}const v={id:"overview",slug:"/"},x="Introduction",j={id:"overview",title:"Introduction",description:"LastMile is the full-stack developer platform to debug, evaluate and improve LLM applications. We make it easy to fine-tune custom evaluators, set up guardrails & monitor app performance.",source:"@site/docs/overview.mdx",sourceDirName:".",slug:"/",permalink:"/",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"overview",slug:"/"},sidebar:"sidebar",next:{title:"Quickstart",permalink:"/quickstart"}},b={},y=[{value:"Meet alBERTa \ud83c\udf41",id:"meet-alberta-",level:2},{value:"Out-of-the-box metrics",id:"out-of-the-box-metrics",level:3},{value:"Design your own metric",id:"design-your-own-metric",level:2},{value:"Explore our guides",id:"explore-our-guides",level:2}];function N(e){const t={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,s.jsxs)(t.p,{children:["LastMile is the full-stack developer platform to debug, evaluate and ",(0,s.jsx)("u",{children:"improve"})," LLM applications. We make it easy to fine-tune custom evaluators, set up guardrails & monitor app performance."]}),"\n",(0,s.jsx)(g,{defaultLanguage:"python",codeBlocks:[{language:"python",code:'from lastmile import LastMile;\nLastMile.eval("Hello world")'},{language:"javascript",label:"node.js",code:"const { LastMile } = require('lastmile');\nLastMile.eval(\"Hello world\");"}]}),"\n",(0,s.jsx)(t.h2,{id:"meet-alberta-",children:"Meet alBERTa \ud83c\udf41"}),"\n",(0,s.jsx)(t.p,{children:"alBERTa is a family of small language models designed for evaluation. They are optimized to be:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"small"})," -- 400M parameter entailment model"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"fast"})," -- can run inference on CPU in < 300ms"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"customizable"})," -- fine-tune for custom evaluation tasks"]}),"\n"]}),"\n",(0,s.jsxs)(m,{className:"alberta-grid",children:[(0,s.jsx)(u,{href:"/autoeval/models",title:"alBERTa-512 \ud83c\udf41",description:"2048 token context, specialized for evaluation tasks (like faithfulness), and gives a numeric 0->1 score.",backgroundColor:"#F5F5F5",className:"custom-card model-512"}),(0,s.jsx)(u,{href:"/autoeval/models",title:"alBERTa-LC-8k \ud83c\udf41",description:"Long-context window variant that can scale to 128k+ tokens using a scaled dot-product attention layer",backgroundColor:"#F5F5F5",className:"custom-card model-8k"})]}),"\n",(0,s.jsx)(t.h3,{id:"out-of-the-box-metrics",children:"Out-of-the-box metrics"}),"\n",(0,s.jsxs)(m,{className:"custom-grid",children:[(0,s.jsx)(u,{href:"/autoeval/metrics",title:"Faithfulness",description:"Measures how adherent or faithful an LLM response is to the provided context. Often used for hallucination detection.",className:"custom-card faithfulness"}),(0,s.jsx)(u,{href:"/autoeval/metrics",title:"Semantic Similarity",description:"Measures semantic similarity between two strings. Often used for context relevance, or input/output relevance, or similarity between a response and ground truth.",className:"custom-card similarity"}),(0,s.jsx)(u,{href:"/autoeval/metrics",title:"Summarization Quality",description:"Quantify the quality of a summarization response.",className:"custom-card summarization"}),(0,s.jsx)(u,{href:"/autoeval/metrics",title:"Toxicity",description:"Quantify the toxicity level in an LLM response.",className:"custom-card toxicity"}),(0,s.jsx)(u,{href:"/autoeval/metrics",title:"More",description:"Explore other metrics available in AutoEval, or keep reading to design your own metric.",className:"custom-card"})]}),"\n",(0,s.jsx)(t.h2,{id:"design-your-own-metric",children:"Design your own metric"}),"\n",(0,s.jsxs)(m,{className:"custom-grid customize",children:[(0,s.jsx)(u,{href:"/autoeval/datasets",title:"1. Create Datasets",description:"Upload and manage application data for running and training evals, and generate synthetic labels.",className:"custom-card datasets"}),(0,s.jsx)(u,{href:"/autoeval/labeling",title:"2. LLM Judge Active Labeling",description:"Generate high-quality labels for your data using LLM Judge with human-in-the-loop",className:"custom-card labeling"}),(0,s.jsx)(u,{href:"/autoeval/fine-tune",title:"3. Fine-tune Models",description:"Use the AutoEval fine-tuning service to develop custom metrics for your application.",className:"custom-card fine-tune"}),(0,s.jsx)(u,{href:"/autoeval/models",title:"4. Run Evals",description:"Compute metrics by running high-performance inference on a prebuilt or fine-tuned model.",className:"custom-card models"})]}),"\n",(0,s.jsx)(t.h2,{id:"explore-our-guides",children:"Explore our guides"}),"\n",(0,s.jsxs)(m,{className:"guides-grid",children:[(0,s.jsx)(u,{href:"/guides/rag-evaluation",title:"Retrieval systems",description:"Evaluate a RAG application for hallucination, relevance and a custom brand tone metric.",className:"custom-card"}),(0,s.jsx)(u,{href:"/guides/multi-agent-evaluation",title:"Multi-agent applications",description:"Evaluate end-to-end and intermediate step metrics for a compound AI system.",className:"custom-card"}),(0,s.jsx)(u,{href:"/guides/guardrails-guide",title:"Real-time guardrails",description:"Use alBERTa \ud83c\udf41 model inference for real-time use-cases, like guardrails.",className:"custom-card"})]})]})}function k(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(N,{...e})}):N(e)}},79402:(e,t,a)=>{"use strict";var s=a(38193);!!s.default.canUseDOM&&navigator.platform.startsWith("Mac"),!!s.default.canUseDOM&&navigator.platform.startsWith("Win")},4865:(e,t,a)=>{"use strict";a.d(t,{A:()=>h});var s=a(96540),i=a(18215),r=a(23104),n=a(47751),l=a(92303);const o={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var c=a(74848);function u(e){let{className:t,block:a,selectedValue:s,selectValue:n,tabValues:l}=e;const u=[],{blockElementScrollPositionUntilNextRender:d}=(0,r.a_)(),m=e=>{const t=e.currentTarget,a=u.indexOf(t),i=l[a].value;i!==s&&(d(t),n(i))},h=e=>{let t=null;switch(e.key){case"Enter":m(e);break;case"ArrowRight":{const a=u.indexOf(e.currentTarget)+1;t=u[a]??u[0];break}case"ArrowLeft":{const a=u.indexOf(e.currentTarget)-1;t=u[a]??u[u.length-1];break}}t?.focus()};return(0,c.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":a},t),children:l.map((e=>{let{value:t,label:a,attributes:r}=e;return(0,c.jsx)("li",{role:"tab",tabIndex:s===t?0:-1,"aria-selected":s===t,ref:e=>u.push(e),onKeyDown:h,onClick:m,...r,className:(0,i.A)("tabs__item",o.tabItem,r?.className,{"tabs__item--active":s===t}),children:a??t},t)}))})}function d(e){let{lazy:t,children:a,selectedValue:r}=e;const n=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=n.find((e=>e.props.value===r));return e?(0,s.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,c.jsx)("div",{className:"margin-top--md",children:n.map(((e,t)=>(0,s.cloneElement)(e,{key:t,hidden:e.props.value!==r})))})}function m(e){const t=(0,n.u)(e);return(0,c.jsxs)("div",{className:(0,i.A)("tabs-container",o.tabList),children:[(0,c.jsx)(u,{...t,...e}),(0,c.jsx)(d,{...t,...e})]})}function h(e){const t=(0,l.default)();return(0,c.jsx)(m,{...e,children:(0,n.v)(e.children)},String(t))}},46942:(e,t)=>{var a;!function(){"use strict";var s={}.hasOwnProperty;function i(){for(var e="",t=0;t<arguments.length;t++){var a=arguments[t];a&&(e=n(e,r(a)))}return e}function r(e){if("string"==typeof e||"number"==typeof e)return e;if("object"!=typeof e)return"";if(Array.isArray(e))return i.apply(null,e);if(e.toString!==Object.prototype.toString&&!e.toString.toString().includes("[native code]"))return e.toString();var t="";for(var a in e)s.call(e,a)&&e[a]&&(t=n(t,a));return t}function n(e,t){return t?e?e+" "+t:e+t:e}e.exports?(i.default=i,e.exports=i):void 0===(a=function(){return i}.apply(t,[]))||(e.exports=a)}()}}]);