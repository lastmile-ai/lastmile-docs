/*! For license information please see 3d9c95a4.b66b1a7d.js.LICENSE.txt */
(self.webpackChunklastmile_docs=self.webpackChunklastmile_docs||[]).push([[279],{32335:(e,t,r)=>{"use strict";r.r(t),r.d(t,{assets:()=>h,contentTitle:()=>m,default:()=>v,frontMatter:()=>d,metadata:()=>p,toc:()=>f});var a=r(74848),i=r(28453),s=(r(4865),r(19365),r(79402),r(96540),r(46942)),o=r.n(s);const n={card:"card_kyB9",cardTitle:"cardTitle_WmU7",icon:"icon_xMNd"};function l(e){let{href:t,title:r,description:i,backgroundColor:s,className:l}=e;return(0,a.jsxs)("a",{href:t,className:o()(n.card,l),style:{backgroundColor:s},children:[(0,a.jsxs)("h3",{className:n.cardTitle,children:[r,(0,a.jsx)("span",{className:n.icon,children:">"})]}),(0,a.jsx)("p",{children:i})]})}const c={grid:"grid_qyhz"};function u(e){let{children:t,className:r}=e;return(0,a.jsx)("div",{className:o()(c.grid,r),children:t})}const d={id:"app_home"},m="AutoEval developer platform",p={id:"app_home",title:"app_home",description:"LastMile is the full-stack developer platform to debug, evaluate and improve LLM applications. We make it easy to fine-tune custom evaluators, set up guardrails & monitor app performance.",source:"@site/docs/home.md",sourceDirName:".",slug:"/app_home",permalink:"/app_home",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"app_home"},sidebar:"sidebar",previous:{title:"LastMile AI",permalink:"/"},next:{title:"Evaluation and Guardrails Overview",permalink:"/overview"}},h={},f=[{value:"Developer quickstart",id:"developer-quickstart",level:2},{value:"Meet alBERTa \ud83c\udf41",id:"meet-alberta-",level:2},{value:"Out-of-the-box metrics",id:"out-of-the-box-metrics",level:3},{value:"Design your own metric",id:"design-your-own-metric",level:2},{value:"Explore our guides",id:"explore-our-guides",level:2}];function g(e){const t={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"autoeval-developer-platform",children:"AutoEval developer platform"})}),"\n",(0,a.jsx)(t.p,{children:"LastMile is the full-stack developer platform to debug, evaluate and improve LLM applications. We make it easy to fine-tune custom evaluators, set up guardrails & monitor app performance."}),"\n",(0,a.jsx)(t.h2,{id:"developer-quickstart",children:"Developer quickstart"}),"\n",(0,a.jsx)(t.h2,{id:"meet-alberta-",children:"Meet alBERTa \ud83c\udf41"}),"\n",(0,a.jsx)(t.p,{children:"alBERTa is a family of small language models designed for evaluation. They are optimized to be:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"small"})," -- 400M parameter entailment model"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"fast"})," -- can run inference on CPU in < 300ms"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"customizable"})," -- fine-tune for custom evaluation tasks"]}),"\n"]}),"\n",(0,a.jsxs)(u,{className:"custom-grid",children:[(0,a.jsx)(l,{href:"/alberta",title:"alBERTa-512 \ud83c\udf41",description:"2048 token context, specialized for evaluation tasks (like faithfulness), and gives a numeric 0->1 score.",backgroundColor:"#F5F5F5",className:"custom-card"}),(0,a.jsx)(l,{href:"/alberta",title:"alBERTa-LC-8k \ud83c\udf41",description:"Long-context window variant that can scale to 128k+ tokens using a scaled dot-product attention layer",backgroundColor:"#F5F5F5",className:"custom-card"})]}),"\n",(0,a.jsx)(t.h3,{id:"out-of-the-box-metrics",children:"Out-of-the-box metrics"}),"\n",(0,a.jsx)(t.p,{children:"Batteries-included evaluation metrics covering common AI application types, such as RAG and multi-agent compound AI systems."}),"\n",(0,a.jsxs)(u,{className:"custom-grid",children:[(0,a.jsx)(l,{href:"/metrics",title:"Faithfulness",description:"Measures how adherent or faithful an LLM response is to the provided context. Often used for hallucination detection.",backgroundColor:"#F1F1F1",className:"custom-card"}),(0,a.jsx)(l,{href:"/metrics",title:"Semantic Similarity",description:"Measures semantic similarity between two strings. Often used for context relevance, or input/output relevance, or similarity between a response and ground truth.",backgroundColor:"#F1F1F1",className:"custom-card"}),(0,a.jsx)(l,{href:"/metrics",title:"Summarization Quality",description:"Quantify the quality of a summarization response.",backgroundColor:"#F1F1F1",className:"custom-card"}),(0,a.jsx)(l,{href:"/metrics",title:"Toxicity",description:"Quantify the toxicity level in an LLM response.",backgroundColor:"#F1F1F1",className:"custom-card"}),(0,a.jsx)(l,{href:"/metrics",title:"More",description:"Explore other metrics available in AutoEval, or keep reading to design your own metric.",backgroundColor:"#F7F7F7",className:"custom-card"})]}),"\n",(0,a.jsx)(t.h2,{id:"design-your-own-metric",children:"Design your own metric"}),"\n",(0,a.jsx)(t.p,{children:"Use the fine-tuning service to design your own evaluators that represent custom criteria for your app quality."}),"\n",(0,a.jsxs)(u,{className:"custom-grid",children:[(0,a.jsx)(l,{href:"/datasets",title:"Datasets",description:"Upload and manage application data for running and training evals",backgroundColor:"#FBF6F7",className:"custom-card"}),(0,a.jsx)(l,{href:"/llm_judge",title:"LLM Judge",description:"Generate high-quality labels for your data using a mixture of LLM Judge with human-in-the-loop",backgroundColor:"#F2F9FF",className:"custom-card"}),(0,a.jsx)(l,{href:"/fine_tune",title:"Fine-tune",description:"Use the AutoEval fine-tuning service to develop custom metrics for your application.",backgroundColor:"#EFFFF4",className:"custom-card"}),(0,a.jsx)(l,{href:"/serve",title:"Run Evals",description:"Compute metrics by running high-performance inference on a prebuilt or fine-tuned model.",backgroundColor:"#F7EFFF",className:"custom-card"})]}),"\n",(0,a.jsx)(t.h2,{id:"explore-our-guides",children:"Explore our guides"}),"\n",(0,a.jsxs)(u,{className:"custom-grid",children:[(0,a.jsx)(l,{href:"/retrieval_systems",title:"Retrieval systems",description:"Evaluate a RAG application for hallucination, relevance and a custom brand tone metric.",className:"custom-card"}),(0,a.jsx)(l,{href:"/multi_agent",title:"Multi-agent applications",description:"Evaluate end-to-end and intermediate step metrics for a compound AI system.",className:"custom-card"}),(0,a.jsx)(l,{href:"/realtime_guardrails",title:"Real-time guardrails",description:"Use alBERTa \ud83c\udf41 model inference for real-time use-cases, like guardrails.",className:"custom-card"})]})]})}function v(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(g,{...e})}):g(e)}},79402:(e,t,r)=>{"use strict";var a=r(38193);!!a.default.canUseDOM&&navigator.platform.startsWith("Mac"),!!a.default.canUseDOM&&navigator.platform.startsWith("Win")},4865:(e,t,r)=>{"use strict";r(96540),r(92303);r(74848)},46942:(e,t)=>{var r;!function(){"use strict";var a={}.hasOwnProperty;function i(){for(var e="",t=0;t<arguments.length;t++){var r=arguments[t];r&&(e=o(e,s(r)))}return e}function s(e){if("string"==typeof e||"number"==typeof e)return e;if("object"!=typeof e)return"";if(Array.isArray(e))return i.apply(null,e);if(e.toString!==Object.prototype.toString&&!e.toString.toString().includes("[native code]"))return e.toString();var t="";for(var r in e)a.call(e,r)&&e[r]&&(t=o(t,r));return t}function o(e,t){return t?e?e+" "+t:e+t:e}e.exports?(i.default=i,e.exports=i):void 0===(r=function(){return i}.apply(t,[]))||(e.exports=r)}()}}]);